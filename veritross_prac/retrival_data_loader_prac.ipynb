{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Install jsonlines, Numpy\n",
    "```\n",
    "pip install jsonlines numpy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1-2. Install beir Library\n",
    "1) Create Virtual Environments\n",
    "```\n",
    "python -m venv {name of virtual environment}\n",
    "python -m venv beir_env\n",
    "```\n",
    "\n",
    "2) Activate VE in bash\n",
    "```\n",
    "source {name of virtual environment}/Scripts/activate\n",
    "source beir_env/Scripts/activate\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Triviaqa Dataset Download\n",
    "```\n",
    "wget http://nlp.cs.washington.edu/triviaqa/data/triviaqa-rc.tar.gz\n",
    "tar -xvf triviaqa-rc.tar.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Import Needed Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting beir\n",
      "  Using cached beir-2.0.0-py3-none-any.whl\n",
      "Collecting sentence-transformers (from beir)\n",
      "  Using cached sentence_transformers-3.0.0-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting pytrec-eval (from beir)\n",
      "  Using cached pytrec_eval-0.5-cp311-cp311-win_amd64.whl\n",
      "Collecting faiss-cpu (from beir)\n",
      "  Using cached faiss_cpu-1.8.0-cp311-cp311-win_amd64.whl.metadata (3.8 kB)\n",
      "Collecting elasticsearch==7.9.1 (from beir)\n",
      "  Using cached elasticsearch-7.9.1-py2.py3-none-any.whl.metadata (8.0 kB)\n",
      "Collecting datasets (from beir)\n",
      "  Using cached datasets-2.19.1-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: urllib3>=1.21.1 in c:\\users\\jenny\\anaconda3\\lib\\site-packages (from elasticsearch==7.9.1->beir) (2.0.7)\n",
      "Requirement already satisfied: certifi in c:\\users\\jenny\\anaconda3\\lib\\site-packages (from elasticsearch==7.9.1->beir) (2024.2.2)\n",
      "Requirement already satisfied: filelock in c:\\users\\jenny\\anaconda3\\lib\\site-packages (from datasets->beir) (3.13.1)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\jenny\\anaconda3\\lib\\site-packages (from datasets->beir) (1.26.4)\n",
      "Requirement already satisfied: pyarrow>=12.0.0 in c:\\users\\jenny\\anaconda3\\lib\\site-packages (from datasets->beir) (14.0.2)\n",
      "Collecting pyarrow-hotfix (from datasets->beir)\n",
      "  Using cached pyarrow_hotfix-0.6-py3-none-any.whl.metadata (3.6 kB)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in c:\\users\\jenny\\anaconda3\\lib\\site-packages (from datasets->beir) (0.3.7)\n",
      "Requirement already satisfied: pandas in c:\\users\\jenny\\anaconda3\\lib\\site-packages (from datasets->beir) (2.1.4)\n",
      "Requirement already satisfied: requests>=2.19.0 in c:\\users\\jenny\\anaconda3\\lib\\site-packages (from datasets->beir) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in c:\\users\\jenny\\anaconda3\\lib\\site-packages (from datasets->beir) (4.65.0)\n",
      "Collecting xxhash (from datasets->beir)\n",
      "  Using cached xxhash-3.4.1-cp311-cp311-win_amd64.whl.metadata (12 kB)\n",
      "Collecting multiprocess (from datasets->beir)\n",
      "  Using cached multiprocess-0.70.16-py311-none-any.whl.metadata (7.2 kB)\n",
      "Requirement already satisfied: fsspec<=2024.3.1,>=2023.1.0 in c:\\users\\jenny\\anaconda3\\lib\\site-packages (from fsspec[http]<=2024.3.1,>=2023.1.0->datasets->beir) (2023.10.0)\n",
      "Requirement already satisfied: aiohttp in c:\\users\\jenny\\anaconda3\\lib\\site-packages (from datasets->beir) (3.9.3)\n",
      "Collecting huggingface-hub>=0.21.2 (from datasets->beir)\n",
      "  Using cached huggingface_hub-0.23.2-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: packaging in c:\\users\\jenny\\anaconda3\\lib\\site-packages (from datasets->beir) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\jenny\\anaconda3\\lib\\site-packages (from datasets->beir) (6.0.1)\n",
      "Collecting transformers<5.0.0,>=4.34.0 (from sentence-transformers->beir)\n",
      "  Using cached transformers-4.41.1-py3-none-any.whl.metadata (43 kB)\n",
      "Collecting torch>=1.11.0 (from sentence-transformers->beir)\n",
      "  Using cached torch-2.3.0-cp311-cp311-win_amd64.whl.metadata (26 kB)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\jenny\\anaconda3\\lib\\site-packages (from sentence-transformers->beir) (1.2.2)\n",
      "Requirement already satisfied: scipy in c:\\users\\jenny\\anaconda3\\lib\\site-packages (from sentence-transformers->beir) (1.11.4)\n",
      "Requirement already satisfied: Pillow in c:\\users\\jenny\\anaconda3\\lib\\site-packages (from sentence-transformers->beir) (10.2.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\jenny\\anaconda3\\lib\\site-packages (from aiohttp->datasets->beir) (1.2.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\jenny\\anaconda3\\lib\\site-packages (from aiohttp->datasets->beir) (23.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\jenny\\anaconda3\\lib\\site-packages (from aiohttp->datasets->beir) (1.4.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\jenny\\anaconda3\\lib\\site-packages (from aiohttp->datasets->beir) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\jenny\\anaconda3\\lib\\site-packages (from aiohttp->datasets->beir) (1.9.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\jenny\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.21.2->datasets->beir) (4.9.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\jenny\\anaconda3\\lib\\site-packages (from requests>=2.19.0->datasets->beir) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\jenny\\anaconda3\\lib\\site-packages (from requests>=2.19.0->datasets->beir) (3.4)\n",
      "Requirement already satisfied: sympy in c:\\users\\jenny\\anaconda3\\lib\\site-packages (from torch>=1.11.0->sentence-transformers->beir) (1.12)\n",
      "Requirement already satisfied: networkx in c:\\users\\jenny\\anaconda3\\lib\\site-packages (from torch>=1.11.0->sentence-transformers->beir) (3.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\jenny\\anaconda3\\lib\\site-packages (from torch>=1.11.0->sentence-transformers->beir) (3.1.3)\n",
      "Collecting mkl<=2021.4.0,>=2021.1.1 (from torch>=1.11.0->sentence-transformers->beir)\n",
      "  Using cached mkl-2021.4.0-py2.py3-none-win_amd64.whl.metadata (1.4 kB)\n",
      "Requirement already satisfied: colorama in c:\\users\\jenny\\anaconda3\\lib\\site-packages (from tqdm>=4.62.1->datasets->beir) (0.4.6)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\jenny\\anaconda3\\lib\\site-packages (from transformers<5.0.0,>=4.34.0->sentence-transformers->beir) (2023.10.3)\n",
      "Collecting tokenizers<0.20,>=0.19 (from transformers<5.0.0,>=4.34.0->sentence-transformers->beir)\n",
      "  Using cached tokenizers-0.19.1-cp311-none-win_amd64.whl.metadata (6.9 kB)\n",
      "Collecting safetensors>=0.4.1 (from transformers<5.0.0,>=4.34.0->sentence-transformers->beir)\n",
      "  Using cached safetensors-0.4.3-cp311-none-win_amd64.whl.metadata (3.9 kB)\n",
      "Collecting dill<0.3.9,>=0.3.0 (from datasets->beir)\n",
      "  Using cached dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\jenny\\anaconda3\\lib\\site-packages (from pandas->datasets->beir) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\jenny\\anaconda3\\lib\\site-packages (from pandas->datasets->beir) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in c:\\users\\jenny\\anaconda3\\lib\\site-packages (from pandas->datasets->beir) (2023.3)\n",
      "Requirement already satisfied: joblib>=1.1.1 in c:\\users\\jenny\\anaconda3\\lib\\site-packages (from scikit-learn->sentence-transformers->beir) (1.2.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\jenny\\anaconda3\\lib\\site-packages (from scikit-learn->sentence-transformers->beir) (2.2.0)\n",
      "Collecting intel-openmp==2021.* (from mkl<=2021.4.0,>=2021.1.1->torch>=1.11.0->sentence-transformers->beir)\n",
      "  Using cached intel_openmp-2021.4.0-py2.py3-none-win_amd64.whl.metadata (1.2 kB)\n",
      "Collecting tbb==2021.* (from mkl<=2021.4.0,>=2021.1.1->torch>=1.11.0->sentence-transformers->beir)\n",
      "  Using cached tbb-2021.12.0-py3-none-win_amd64.whl.metadata (1.1 kB)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\jenny\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->datasets->beir) (1.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\jenny\\anaconda3\\lib\\site-packages (from jinja2->torch>=1.11.0->sentence-transformers->beir) (2.1.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\jenny\\anaconda3\\lib\\site-packages (from sympy->torch>=1.11.0->sentence-transformers->beir) (1.3.0)\n",
      "Using cached elasticsearch-7.9.1-py2.py3-none-any.whl (219 kB)\n",
      "Using cached datasets-2.19.1-py3-none-any.whl (542 kB)\n",
      "Using cached faiss_cpu-1.8.0-cp311-cp311-win_amd64.whl (14.5 MB)\n",
      "Using cached sentence_transformers-3.0.0-py3-none-any.whl (224 kB)\n",
      "Using cached huggingface_hub-0.23.2-py3-none-any.whl (401 kB)\n",
      "Using cached torch-2.3.0-cp311-cp311-win_amd64.whl (159.8 MB)\n",
      "Using cached transformers-4.41.1-py3-none-any.whl (9.1 MB)\n",
      "Using cached multiprocess-0.70.16-py311-none-any.whl (143 kB)\n",
      "Using cached dill-0.3.8-py3-none-any.whl (116 kB)\n",
      "Using cached pyarrow_hotfix-0.6-py3-none-any.whl (7.9 kB)\n",
      "Using cached xxhash-3.4.1-cp311-cp311-win_amd64.whl (29 kB)\n",
      "Using cached mkl-2021.4.0-py2.py3-none-win_amd64.whl (228.5 MB)\n",
      "Using cached intel_openmp-2021.4.0-py2.py3-none-win_amd64.whl (3.5 MB)\n",
      "Using cached tbb-2021.12.0-py3-none-win_amd64.whl (286 kB)\n",
      "Using cached safetensors-0.4.3-cp311-none-win_amd64.whl (287 kB)\n",
      "Using cached tokenizers-0.19.1-cp311-none-win_amd64.whl (2.2 MB)\n",
      "Installing collected packages: tbb, intel-openmp, xxhash, safetensors, pytrec-eval, pyarrow-hotfix, mkl, faiss-cpu, elasticsearch, dill, torch, multiprocess, huggingface-hub, tokenizers, transformers, datasets, sentence-transformers, beir\n",
      "  Attempting uninstall: dill\n",
      "    Found existing installation: dill 0.3.7\n",
      "    Uninstalling dill-0.3.7:\n",
      "      Successfully uninstalled dill-0.3.7\n",
      "Successfully installed beir-2.0.0 datasets-2.19.1 dill-0.3.8 elasticsearch-7.9.1 faiss-cpu-1.8.0 huggingface-hub-0.23.2 intel-openmp-2021.4.0 mkl-2021.4.0 multiprocess-0.70.16 pyarrow-hotfix-0.6 pytrec-eval-0.5 safetensors-0.4.3 sentence-transformers-3.0.0 tbb-2021.12.0 tokenizers-0.19.1 torch-2.3.0 transformers-4.41.1 xxhash-3.4.1\n"
     ]
    }
   ],
   "source": [
    "!pip install beir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jenny\\anaconda3\\Lib\\site-packages\\beir\\datasets\\data_loader.py:2: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import jsonlines\n",
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "from beir.datasets.data_loader import GenericDataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Main Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### retrieve_eval_data_loader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_eval_data_loader(data_name, query_file, corpus_file,qrels_file):\n",
    "\n",
    "    data_path = f\"datasets/{data_name}/\"\n",
    "    \n",
    "    print(\"=====> Loading QA Dataset\")\n",
    "    print(\"=====> Loading Pool of Documents\")\n",
    "    corpus, queries, qrels = GenericDataLoader(\n",
    "        data_folder = data_path,\\\n",
    "        corpus_file = corpus_file,\\\n",
    "        query_file = query_file,\\\n",
    "        ).load(split=qrels_file) # or split = \"train\" or \"dev\"\n",
    "    \n",
    "\n",
    "    \n",
    "    return corpus, queries, qrels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### custom_data_loaer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_data_loader(args):\n",
    "\n",
    "    if args.data_name == 'triviaqa':\n",
    "        questions = []\n",
    "        answers = []\n",
    "        with jsonlines.open(args.qa_data) as f:\n",
    "            for line in f.iter():\n",
    "                questions.append(line[\"query\"])\n",
    "                answers.append(line[\"answers\"])\n",
    "    \n",
    "        retrieval_queries = {}\n",
    "        for i in tqdm(range(len(questions)), desc=\"QA Data Preprocessing\"):\n",
    "            question = questions[i]\n",
    "            qa_id = str(args.data_name) + \"_\" + str(i)\n",
    "            retrieval_queries[qa_id] = question\n",
    "    \n",
    "        titles = []\n",
    "        texts = []\n",
    "        with jsonlines.open(args.documents_pool) as f:\n",
    "            for line in f.iter():\n",
    "                texts.append(line[\"text\"])\n",
    "                titles.append(line[\"title\"])\n",
    "    \n",
    "        retrieval_corpus = {}\n",
    "        for i in tqdm(range(len(titles)), desc= \"Documents_Pool Preprocessing\"):\n",
    "            json_obj = {}\n",
    "            json_obj[\"title\"] = titles[i]\n",
    "            json_obj[\"text\"] = texts[i]\n",
    "            retrieval_corpus[str(i)] = json_obj\n",
    "\n",
    "    return retrieval_queries, retrieval_corpus, questions, answers, titles, texts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### make_new_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_new_dataset(args, retrieved_doc):\n",
    "        \n",
    "        _,_, questions, answers, titles,texts = custom_data_loader(args)\n",
    "        print(\"=====> Starting Construction of Dataset\")\n",
    "\n",
    "        sorted_idxs = []\n",
    "        sorted_scores = []\n",
    "\n",
    "        for i in range(len(titles)):\n",
    "            scores_i = np.array(list(retrieved_doc['{}_{}'.format(args.data_name, i)].values()))\n",
    "            sorted_idx = np.argsort(scores_i)[::-1]\n",
    "            keys = list(retrieved_doc['{}_{}'.format(args.data_name, i)].keys())\n",
    "\n",
    "            sorted_idxs_i = []\n",
    "            sorted_scores_i = []\n",
    "            for j in range(min(len(scores_i), args.num_retrieval)):\n",
    "                sorted_idxs_i.append(int(keys[sorted_idx[j]]))\n",
    "                sorted_scores_i.append(scores_i[sorted_idx[j]])\n",
    "\n",
    "            sorted_idxs.append(sorted_idxs_i)\n",
    "            sorted_scores.append(sorted_scores_i)\n",
    "\n",
    "        res = []\n",
    "        for i in range(len(questions)):\n",
    "            new_item = {}\n",
    "            new_item['question'] = questions[i]\n",
    "            new_item['answer'] = answers[i]\n",
    "\n",
    "            ctxs = []\n",
    "            for j in range(len(sorted_idxs[i])):\n",
    "                ctx = {}\n",
    "                ctx['id'] = sorted_idxs[i][j]\n",
    "                ctx['title'] = titles[sorted_idxs[i][j]]\n",
    "                ctx['text'] = texts[sorted_idxs[i][j]]\n",
    "                ctx['score'] = sorted_scores[i][j]\n",
    "                ctxs.append(ctx)\n",
    "            new_item['contexts'] = ctxs\n",
    "            res.append(new_item)\n",
    "\n",
    "        if not os.path.exists(args.output_folder):\n",
    "            os.makedirs(args.output_folder)\n",
    "        \n",
    "        print(\"=====> All Procedure is finished!\")\n",
    "        with open(f\"{args.output_folder}/{args.data_name}/{args.retrieval_method}_retrieved_docs.doc\",'w') as writer:\n",
    "            writer.write(json.dumps(res, indent=4, ensure_ascii=False) + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Args:\n",
    "    data_name = 'triviaqa'\n",
    "    qa_data = '/content/qa/wikipedia-dev.json'  # TriviaQA QA 데이터 경로\n",
    "    documents_pool = '/content/documents_pool.json'  # 문서 풀 데이터 경로\n",
    "    output_folder = 'output_folder'\n",
    "    retrieval_method = 'retrieval_method'\n",
    "    num_retrieval = 5\n",
    "\n",
    "args = Args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieved_doc = {f'triviaqa_{i}': {str(j): np.random.random() for j in range(10)} for i in range(5)}  # 예시 데이터\n",
    "\n",
    "make_new_dataset(args, retrieved_doc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
